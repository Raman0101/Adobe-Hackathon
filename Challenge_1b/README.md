# ğŸ§  Persona-Driven Document Intelligence â€“ Challenge 1b

A fast, containerized solution for **Adobe India Hackathon 2025 - Challenge 1b**.  
This system extracts and ranks the most relevant sections from a PDF document collection based on a **persona** and their **job-to-be-done** â€” all within **1GB model size**, **CPU-only**, and under **60 seconds** runtime.

---

## ğŸŒ Overview

This project performs:

- âœ… **Query parsing** from persona & job-to-be-done  
- ğŸ“„ **PDF chunking** using `pdfplumber`  
- ğŸ§  **Text embeddings** using `sentence-transformers`  
- ğŸ¯ **Chunk ranking** via cosine similarity  
- ğŸ“¤ **Top-k output generation** in JSON format  

---

## ğŸ› ï¸ Folder Structure

```plaintext
Challenge_1b/
â”œâ”€â”€ Collection_1/
â”‚   â”œâ”€â”€ PDFs/
â”‚   â”œâ”€â”€ challenge1b_input.json
â”‚   â””â”€â”€ challenge1b_output.json
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ main.py
â”‚   â”œâ”€â”€ input_parser.py
â”‚   â”œâ”€â”€ pdf_reader.py
â”‚   â”œâ”€â”€ chunk_embedder.py
â”‚   â”œâ”€â”€ ranker.py
â”‚   â””â”€â”€ output_writer.py
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸ“¥ Input Format

### `challenge1b_input.json`

```json
{
  "challenge_info": {
    "challenge_id": "1b",
    "collection_id": "Collection_1"
  },
  "persona": {
    "role": "Data Scientist"
  },
  "job_to_be_done": {
    "task": "Understand infrastructure pain points"
  },
  "documents": [
    "infra_guide.pdf"
  ]
}
```

---

## ğŸ“¤ Output Format

### `challenge1b_output.json`

```json
{
  "persona": "Data Scientist",
  "job_to_be_done": "Understand infrastructure pain points",
  "answers": [
    {
      "document": "infra_guide.pdf",
      "page_number": 2,
      "text": "The biggest challenge in infrastructure is balancing compute and cost..."
    }
  ]
}
```

---

## ğŸ³ Docker Instructions

### ğŸ”§ Build the Image

```bash
docker build -t persona-doc-intel .
```

### ğŸš€ Run the Container

```bash
docker run --rm \
  -v $(pwd)/Collection_1:/app/Collection_1 \
  persona-doc-intel python src/main.py Collection_1
```

---

## ğŸ§  How It Works

| Step               | Description                                                                 |
|--------------------|-----------------------------------------------------------------------------|
| `input_parser.py`  | Parses `challenge1b_input.json` for persona and task                        |
| `pdf_reader.py`    | Extracts 150-word overlapping chunks from PDFs using `pdfplumber`           |
| `chunk_embedder.py`| Embeds chunks and query using `all-MiniLM-L6-v2` via `sentence-transformers`|
| `ranker.py`        | Ranks chunks by cosine similarity to the query                              |
| `output_writer.py` | Writes top-ranked chunks to final output JSON                               |

---

## ğŸ“¦ Dependencies

### `requirements.txt`

```txt
pdfplumber
numpy
scikit-learn
sentence-transformers==2.2.2
huggingface_hub==0.14.1
```

---

## ğŸ³ Dockerfile

```dockerfile
FROM python:3.10-slim

WORKDIR /app
ENV PYTHONPATH=/app

COPY requirements.txt .
RUN pip install --no-cache-dir torch==2.2.2+cpu -f https://download.pytorch.org/whl/torch_stable.html
RUN pip install --no-cache-dir -r requirements.txt

COPY . .

CMD ["python", "src/main.py"]
```

---

## âš¡ Performance Snapshot

```plaintext
[INFO] âœ… Loaded input configuration
[INFO] ğŸ“„ Extracting chunks from PDFs in Collection_1/PDFs...
[INFO] ğŸ§  Embedding chunks...
[INFO] ğŸ” Embedding query...
[INFO] ğŸ¯ Ranking chunks...
[INFO] âœ… Output written to Collection_1/challenge1b_output.json
Total time: ~35 seconds on CPU (â‰¤1GB RAM)
```

---

## âœ… Features

- 100% CPU-only inference  
- Supports multiple PDFs per collection  
- Compact model: ~80MB  
- Fully Dockerized & offline-safe  
- Compliant with 1GB model and 60s runtime constraints  

---

## ğŸ‘¨â€ğŸ’» Author

**Raman Kumar**  
GitHub: [@Raman0101](https://github.com/Raman0101)

---

## ğŸ™ Acknowledgements

- Adobe India Hackathon Team  
- Sentence Transformers  
- pdfplumber
